{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image Download\n",
    "from google.colab import files\n",
    "from IPython.display import Image\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.backend import image_data_format\n",
    "from keras.utils import conv_utils\n",
    "\n",
    "from keras.activations import relu\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization, Activation, GlobalAveragePooling2D, Add\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D, Convolution2D, DepthwiseConv2D, SeparableConv2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "print('Python version : {}'.format(sys.version))\n",
    "print('TensorFlow version : {}'.format(tf.__version__))\n",
    "print('Keras version : {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD and PREPROCESSING (CIFAR-10)\n",
    "### CIFAR-10 : 10 종류의 카테고리 라벨을 가지는 50,000 개의 32x32 해상도 컬러 트레이닝 이미지와 10,000 개의 테스트 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))\n",
    "\n",
    "print('{} train samples'.format(x_train.shape[0]))\n",
    "print('{} test samples'.format(x_test.shape[0]))\n",
    "\n",
    "\n",
    "# Normalizae x data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Convert class vectors to binary class matrices (ONE-HOT ENCODING)\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# the CIFAR10 images are RGB\n",
    "img_channels = 3\n",
    "\n",
    "# CIFAR10 input shape\n",
    "input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "# Set configurations\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss_function = keras.losses.categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PRACTICE 1] Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('simplecnn.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Analyze above architecture and construct the simple cnn model__\n",
    "<font color=red>Caution : When you construct CNN, do not touch stride and padding size.</font> Just use proper\n",
    "kernel size to match the proposed architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use below classes to construct the model__\n",
    "1. Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', activation=None)\n",
    "   <font color=red>When using this layer as the first layer in a model, provide the keyword argument __'input_shape'__</font>\n",
    "2. MaxPooling2D(pool_size=( , ))\n",
    "3. Flatten()\n",
    "4. Dense(units, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple CNN\n",
    "\n",
    "model1 = Sequential([\n",
    "    Conv2D(8, kernel_size=(5, 5), strides=(1, 1), padding='valid', activation='relu', input_shape=input_shape),\n",
    "    #################### you need to add 7 lines ###############################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set training environment\n",
    "model1.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Training the Simple CNN model\n",
    "simple_CNN = model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                        validation_data=(x_test, y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting final model's valid accuracy\n",
    "score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score1[0]))\n",
    "print('Test accuracy: {}'.format(score1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PRACTICE 2] VGG like + <font color=red>BatchNormalization right after conv layer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('vgglike_BN.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Analyze above architecture and construct VGG like model__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use below classes to construct the model__\n",
    "1. Conv2D(filters, kernel_size, strides=(1, 1), padding='same', activation=None)\n",
    "<font color=red>When using this layer as the first layer in a model, provide the keyword argument __'input_shape'__</font>\n",
    "2. MaxPooling2D(pool_size=( , ))\n",
    "3. AveragePooling2D(pool_size=( , ))\n",
    "4. Flatten()\n",
    "5. Dense(units, activation=None)\n",
    "6. BatchNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG-like\n",
    "\n",
    "model2 = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same',input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    ########################### you need to add 26 lines ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set training environment\n",
    "model2.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Training the Simple CNN model\n",
    "VGG_like = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                      validation_data=(x_test, y_test))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting final model's valid accuracy\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score2[0]))\n",
    "print('Test accuracy: {}'.format(score2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PRACTICE 3] ResNet like + <font color=red>BatchNormalization right after conv layer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(\"resnetlike_BN.png\", width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Analyze above architecture and construct the ResNet like model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use below classes to construct the model__\n",
    "1. Add()([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet-like\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "\n",
    "x = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same',input_shape=input_shape)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(32, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# First residual shortcut\n",
    "############################# you need to store residual value ##################\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(64, (3, 3), activation=None, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "############################ you need to connect the residual shortcut #########\n",
    "\n",
    "################################################################################\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Second residual shortcut\n",
    "############################# you need to store residual value ##################\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(128, (3, 3), activation=None, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "############################ you need to connect the residual shortcut #########\n",
    "\n",
    "################################################################################\n",
    "x = Activation('relu')(x)\n",
    "x = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model3 = Model(inputs=inputs, outputs=outputs)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set training environment\n",
    "model3.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Training the Simple CNN model\n",
    "ResNet_like = model3.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                      validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting final model's valid accuracy\n",
    "score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score3[0]))\n",
    "print('Test accuracy: {}'.format(score3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PRACTICE 4] MobileNetV1 like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('depthwiseconv.jpg', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [figure 1] Standard convolution vs. Depthwise convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('pointwise.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [figure 2] Pointwise convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('Depthwisefigure.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [figure 3] Depthwise Separable convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(\"Depthwisecalculate.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [figure 4] Reduction of # computational costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(\"depthwiseseparation.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [figure 5] Left : Standard convolutions, Right : Depthwise separable convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(\"mobilenetv1like.png\", width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Analyze above architecture and construct the MobileNetV1 like model__\n",
    "### (cf. This model is <font color=red>just the same as VGG like model</font> with <font color=blue>depthwise separable layers</font> except for first and last layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use below classes to construct the model__\n",
    "1. Conv2D(filters, kernel_size, strides=(1, 1), padding='same', activation=None)\n",
    "   <font color=red>When using this layer as the first layer in a model, provide the keyword argument __'input_shape'__</font>\n",
    "2. SeparableConv2D(filters, kernel_size, strides, padding)  \n",
    "3. BatchNormalization()(input)\n",
    "4. Activation('')\n",
    "5. MaxPooling2D(pool_size=( , ))\n",
    "6. AveragePooling2D(pool_size=( , ))\n",
    "7. Add()([x1, x2])\n",
    "8. Flatten()\n",
    "9. Dense(units, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MobileNetV1-like\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "\n",
    "x = Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "\n",
    "# First depthwise separable convolution\n",
    "x = SeparableConv2D(32, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(32, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "####################### you need to complete the full architecture ###########################\n",
    "####################### you have two choices #################################################\n",
    "####################### 1. Add all computation of layers one by one ##########################\n",
    "####################### 2. you may use some depthwise separable function for convenience #####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model4 = Model(inputs=inputs, outputs=outputs)\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set training environment\n",
    "model4.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Training the Simple CNN model\n",
    "MobileNetV1_like = model4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                      validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting final model's valid accuracy\n",
    "score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score4[0]))\n",
    "print('Test accuracy: {}'.format(score4[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
